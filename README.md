# ML-models-comparison

  
## Score meaning  
1: Poor
2: Below Average
3: Average
4: Good
5: Excellent  

## Regression model  
- Indicators:  
    1. Accuracy in General
    2. Training Speed
    3. Prediction Speed
    4. Tolerance to Missing Values
    5. Tolerance to Irrelevant Features
    6. Tolerance to Multicollinearity
    7. Tolerance to Noisy Data
    8. Scalability with Increasing Data
    9. Interpretability
    10. Dealing with Overfitting
    11. Hyperparameter Sensitivity
    12. Feature Engineering Requirement
    13. Assumption of Linearity
    14. Handling of Categorical Variables
    15. Handling of Multimodal Distributions
    16. Ability to Model Non-Linear Relationships
    17. Memory Usage
    18. Parallelization Capability
    19. Ease of Deployment
- Takeaways:
    * Linear Regression & Bayesian Ridge Regression → Best when interpretability is key, but struggles with complex patterns.
    * Decision Tree → Works well with small datasets, interpretable but prone to overfitting.
    * Random Forest → Good balance between performance, robustness, and overfitting control.
    * XGBoost & LightGBM → Most powerful models for structured data but require careful tuning.
    * Lasso Regression → Feature selection built-in but struggles with non-linearity.
    * Gradient Boosting & AdaBoost → Strong predictive performance but computationally expensive.
    * Neural Networks → Most powerful for large data, but expensive and harder to interpret.  
      
## Time-series forecasting model  
- Indicators:  
    1. Accuracy in General
    2. Training Speed
    3. Prediction Speed
    4. Handling Seasonality
    5. Handling Trend
    6. Handling Missing Data
    7. Robustness to Outliers
    8. Scalability with Increasing Data
    9. Interpretability
    10. Handling Multivariate Data
    11. Handling Non-Stationarity
    12. Dealing with Overfitting
    13. Memory Usage
    14. Hyperparameter Sensitivity
    15. Parallelization Capability
    
    
## Todo  
- [ ] explanation of the score